{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textblob in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2021.8.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: textstat in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: pyphen in c:\\users\\dell\\anaconda3\\lib\\site-packages (from textstat) (0.15.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing all the dependencies \n",
    "%pip install beautifulsoup4\n",
    "%pip install selenium\n",
    "%pip install webdriver-manager\n",
    "%pip install textblob\n",
    "%pip install requests\n",
    "%pip install pandas nltk textstat openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries  \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import textstat\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the input file\n",
    "urldata = pd.read_csv('./urls.csv')\n",
    "urldata = urldata.dropna()\n",
    "urls = urldata['URL']\n",
    "urldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 100/100..."
     ]
    }
   ],
   "source": [
    "def getpages(urls : list[str]) -> list[str]:\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=ChromeService( \n",
    "        ChromeDriverManager().install()), options=options) \n",
    "    pages = []\n",
    "    for i, each in enumerate(urls):\n",
    "        print(f'\\rFetching page {i+1}/{len(urls)}...', end='')\n",
    "        driver.get(each)\n",
    "        pages.append(driver.page_source)\n",
    "    return pages\n",
    "\n",
    "pages = getpages(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstring(node) -> str:\n",
    "    children = list(node.children)\n",
    "    retter = ''\n",
    "    if len(children) > 1:\n",
    "        for each in children:\n",
    "            retter += getstring(each)\n",
    "    else:\n",
    "        retter = node.string\n",
    "    return retter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(rawpage: str, elements: list[str] = None) -> str:\n",
    "    if elements is None:\n",
    "        elements = ['p', 'li']\n",
    "    text = []\n",
    "    soup = BeautifulSoup(rawpage, 'html.parser')\n",
    "    article = soup.article\n",
    "    title_tag = soup.find('h1') # Extractring the title of the article\n",
    "    if not title_tag:\n",
    "        title_tag = soup.find('h2')  # <h2> as a fallback\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title Found\"\n",
    "    \n",
    "    if article is not None:\n",
    "        for each in elements:\n",
    "            each_elements = list(filter(lambda x: 'class' not in x.attrs, article.find_all(each)))\n",
    "            each_texts = list(map(lambda x: '\\n'.join(x.strings), each_elements))\n",
    "            text += each_texts \n",
    "    return title+ '\\n' +'\\n'.join(text)\n",
    "try:\n",
    "    os.mkdir('./articles')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pages is a list of raw HTML page strings and urldata is a DataFrame\n",
    "for i, each in enumerate(pages):\n",
    "    text = scrape_page(each)\n",
    "    with open(f'./articles/{urldata.iloc[i, 0]}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords from the StopWords folder\n",
    "stop_words = set()\n",
    "for stopword_file in os.listdir('StopWords'):\n",
    "    with open(os.path.join('StopWords', stopword_file), 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if word:\n",
    "                stop_words.add(word.lower())\n",
    "\n",
    "# Load positive and negative word dictionaries\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "with open('MasterDictionary/positive-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        if word and word.lower() not in stop_words:\n",
    "            positive_words.add(word.lower())\n",
    "\n",
    "with open('MasterDictionary/negative-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        if word and word.lower() not in stop_words:\n",
    "            negative_words.add(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for text analysis\n",
    "def clean_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "def get_word_count(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def get_sentence_count(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def get_avg_sentence_length(text):\n",
    "    words = get_word_count(text)\n",
    "    sentences = get_sentence_count(text)\n",
    "    return words / sentences if sentences != 0 else 0\n",
    "\n",
    "def get_complex_word_count(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n",
    "    return len(complex_words)\n",
    "\n",
    "def get_percentage_complex_words(text):\n",
    "    word_count = get_word_count(text)\n",
    "    complex_word_count = get_complex_word_count(text)\n",
    "    return (complex_word_count / word_count) * 100 if word_count != 0 else 0\n",
    "\n",
    "def get_fog_index(text):\n",
    "    return textstat.gunning_fog(text)\n",
    "\n",
    "def get_syllable_per_word(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    total_syllables = sum(textstat.syllable_count(word) for word in words)\n",
    "    return total_syllables / len(words) if len(words) != 0 else 0\n",
    "\n",
    "def get_avg_word_length(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    return total_length / len(words) if len(words) != 0 else 0\n",
    "\n",
    "def get_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len(pronouns)\n",
    "\n",
    "def get_polarity_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def get_subjectivity_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity\n",
    "\n",
    "def get_positive_score(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    positive_words_count = sum(1 for word in words if word in positive_words)\n",
    "    return positive_words_count\n",
    "\n",
    "def get_negative_score(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    negative_words_count = sum(1 for word in words if word in negative_words)\n",
    "    return negative_words_count\n",
    "\n",
    "def get_polarity_score_calculated(positive_score, negative_score):\n",
    "    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "def get_subjectivity_score_calculated(positive_score, negative_score, word_count):\n",
    "    return (positive_score + negative_score) / (word_count + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the articles and populate the output dataframe\n",
    "output_data = []\n",
    "\n",
    "for index, row in urldata.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    file_path = f'articles/{url_id}.txt'\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                cleaned_text = clean_text(text)\n",
    "                \n",
    "            word_count = get_word_count(cleaned_text)\n",
    "            positive_score = get_positive_score(cleaned_text)\n",
    "            negative_score = get_negative_score(cleaned_text)\n",
    "            analysis = {\n",
    "                'URL_ID': url_id,\n",
    "                'URL': url,\n",
    "                'POSITIVE SCORE': positive_score,\n",
    "                'NEGATIVE SCORE': negative_score,\n",
    "                'POLARITY SCORE': get_polarity_score_calculated(positive_score, negative_score),\n",
    "                'SUBJECTIVITY SCORE': get_subjectivity_score_calculated(positive_score, negative_score, word_count),\n",
    "                'AVG SENTENCE LENGTH': get_avg_sentence_length(cleaned_text),\n",
    "                'PERCENTAGE OF COMPLEX WORDS': get_percentage_complex_words(cleaned_text),\n",
    "                'FOG INDEX': get_fog_index(cleaned_text),\n",
    "                'AVG NUMBER OF WORDS PER SENTENCE': get_avg_sentence_length(cleaned_text),\n",
    "                'COMPLEX WORD COUNT': get_complex_word_count(cleaned_text),\n",
    "                'WORD COUNT': word_count,\n",
    "                'SYLLABLE PER WORD': get_syllable_per_word(cleaned_text),\n",
    "                'PERSONAL PRONOUNS': get_personal_pronouns(cleaned_text),\n",
    "                'AVG WORD LENGTH': get_avg_word_length(cleaned_text)\n",
    "            }\n",
    "            \n",
    "            output_data.append(analysis)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {url_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Convert output_data to a DataFrame\n",
    "df_output = pd.DataFrame(output_data)\n",
    "\n",
    "# Ensure columns are in the correct order as per the output structure\n",
    "columns_order = [\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "]\n",
    "\n",
    "df_output = df_output[columns_order]\n",
    "\n",
    "# Save to the output file\n",
    "output_file = 'Output.xlsx'\n",
    "df_output.to_excel(output_file, index=False)\n",
    "print(\"Text analysis completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
